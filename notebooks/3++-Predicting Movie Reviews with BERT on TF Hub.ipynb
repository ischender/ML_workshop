{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Predicting Movie Reviews with BERT on TF Hub.ipynb","provenance":[{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1573062125704}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Predicting Movie Review Sentiment with BERT on TF Hub"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n","\n","Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n","\n","Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"]},{"cell_type":"code","metadata":{"id":"hsZvic2YxnTz","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cp5wfXDx5SPH","colab_type":"text"},"source":["In addition to the standard libraries we imported above, we'll need to install BERT's python package."]},{"cell_type":"code","metadata":{"id":"jviywGyWyKsA","colab_type":"code","outputId":"517f3de5-b805-4a0b-e359-4432aced536e","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1573068812242,"user_tz":-60,"elapsed":7504,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["!pip install bert-tensorflow"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hhbGEfwgdEtw","colab_type":"code","colab":{}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n","\n","Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","outputId":"02d6e042-b0b5-439e-daa7-2594d2dbbeea","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1573068812244,"user_tz":-60,"elapsed":7442,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = False #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = False #@param {type:\"boolean\"}\n","BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["***** Model output directory: OUTPUT_DIR_NAME *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data"]},{"cell_type":"markdown","metadata":{"id":"MC_w8SRqN0fr","colab_type":"text"},"source":["First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."]},{"cell_type":"code","metadata":{"id":"fom_ff20gyy6","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","  \n","  return train_df, test_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2abfwdn-g135","colab_type":"code","colab":{}},"source":["train, test = download_and_load_datasets()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XA8WHJgzhIZf","colab_type":"text"},"source":["To keep training fast, we'll take a sample of 5000 train and test examples, respectively."]},{"cell_type":"code","metadata":{"id":"lw_F488eixTV","colab_type":"code","colab":{}},"source":["train = train.sample(5000)\n","test = test.sample(5000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"prRQM8pDi8xI","colab_type":"code","outputId":"e604f496-72c2-4f83-fbcf-73dddf94f06a","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1573068857068,"user_tz":-60,"elapsed":52192,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["train.columns"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sentence', 'sentiment', 'polarity'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"source":["For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"source":["DATA_COLUMN = 'sentence'\n","LABEL_COLUMN = 'polarity'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qMWiDtpyQSoU","colab_type":"text"},"source":["To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"]},{"cell_type":"code","metadata":{"id":"IhJSe0QHNG7U","colab_type":"code","outputId":"7efad653-3ed8-4cb1-8f34-b0ee99722791","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1573068860343,"user_tz":-60,"elapsed":55413,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":46,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"z4oFkhpZBDKm","colab_type":"text"},"source":["Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"]},{"cell_type":"code","metadata":{"id":"dsBo6RCtQmwx","colab_type":"code","outputId":"c3438e20-de30-49a1-affc-e71fcbcb6268","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1573068860344,"user_tz":-60,"elapsed":55399,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","outputId":"2724a915-db81-412b-88b4-38284d808473","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1573068899108,"user_tz":-60,"elapsed":94145,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] okay , so i get it . we ' re supposed to be horrified . the idea has been planted . a girl is doing her dad and taking photos of it . call me over the shock - rock genre but i call for the explicit detailing of an act before i can fall for this . but don ' t expect me to watch a soft - porn and become horrified that she is ' doing her father ' . . . i mean hasn ' t that convention become a bit abused in the adult film industry already infiltrate ##d with ' rape , and mole ##station ' porn . . . horror isn ' t what your mind can fool you into [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] okay , so i get it . we ' re supposed to be horrified . the idea has been planted . a girl is doing her dad and taking photos of it . call me over the shock - rock genre but i call for the explicit detailing of an act before i can fall for this . but don ' t expect me to watch a soft - porn and become horrified that she is ' doing her father ' . . . i mean hasn ' t that convention become a bit abused in the adult film industry already infiltrate ##d with ' rape , and mole ##station ' porn . . . horror isn ' t what your mind can fool you into [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3100 1010 2061 1045 2131 2009 1012 2057 1005 2128 4011 2000 2022 14603 1012 1996 2801 2038 2042 8461 1012 1037 2611 2003 2725 2014 3611 1998 2635 7760 1997 2009 1012 2655 2033 2058 1996 5213 1011 2600 6907 2021 1045 2655 2005 1996 13216 17555 1997 2019 2552 2077 1045 2064 2991 2005 2023 1012 2021 2123 1005 1056 5987 2033 2000 3422 1037 3730 1011 22555 1998 2468 14603 2008 2016 2003 1005 2725 2014 2269 1005 1012 1012 1012 1045 2812 8440 1005 1056 2008 4680 2468 1037 2978 16999 1999 1996 4639 2143 3068 2525 29543 2094 2007 1005 9040 1010 1998 16709 20100 1005 22555 1012 1012 1012 5469 3475 1005 1056 2054 2115 2568 2064 7966 2017 2046 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3100 1010 2061 1045 2131 2009 1012 2057 1005 2128 4011 2000 2022 14603 1012 1996 2801 2038 2042 8461 1012 1037 2611 2003 2725 2014 3611 1998 2635 7760 1997 2009 1012 2655 2033 2058 1996 5213 1011 2600 6907 2021 1045 2655 2005 1996 13216 17555 1997 2019 2552 2077 1045 2064 2991 2005 2023 1012 2021 2123 1005 1056 5987 2033 2000 3422 1037 3730 1011 22555 1998 2468 14603 2008 2016 2003 1005 2725 2014 2269 1005 1012 1012 1012 1045 2812 8440 1005 1056 2008 4680 2468 1037 2978 16999 1999 1996 4639 2143 3068 2525 29543 2094 2007 1005 9040 1010 1998 16709 20100 1005 22555 1012 1012 1012 5469 3475 1005 1056 2054 2115 2568 2064 7966 2017 2046 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] in 1993 , with the success of the first season of batman : the animated series , warner brothers commissioned the team responsible for the hit - show with producing a feature - length movie , originally slated for direct - to - video , but bumped up to theatrical status . it would become known as batman : mask of the ph ##anta ##sm . ten years after ph ##anta ##sm , we have had an additional three feature - films released from the boys at the wb , sub - zero , return of the joker , and now , mystery of the bat ##woman joins the family . < br / > < br / > the plot is basic and in many [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] in 1993 , with the success of the first season of batman : the animated series , warner brothers commissioned the team responsible for the hit - show with producing a feature - length movie , originally slated for direct - to - video , but bumped up to theatrical status . it would become known as batman : mask of the ph ##anta ##sm . ten years after ph ##anta ##sm , we have had an additional three feature - films released from the boys at the wb , sub - zero , return of the joker , and now , mystery of the bat ##woman joins the family . < br / > < br / > the plot is basic and in many [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1999 2857 1010 2007 1996 3112 1997 1996 2034 2161 1997 8942 1024 1996 6579 2186 1010 6654 3428 4837 1996 2136 3625 2005 1996 2718 1011 2265 2007 5155 1037 3444 1011 3091 3185 1010 2761 18517 2005 3622 1011 2000 1011 2678 1010 2021 19030 2039 2000 8900 3570 1012 2009 2052 2468 2124 2004 8942 1024 7308 1997 1996 6887 26802 6491 1012 2702 2086 2044 6887 26802 6491 1010 2057 2031 2018 2019 3176 2093 3444 1011 3152 2207 2013 1996 3337 2012 1996 25610 1010 4942 1011 5717 1010 2709 1997 1996 19318 1010 1998 2085 1010 6547 1997 1996 7151 10169 9794 1996 2155 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5436 2003 3937 1998 1999 2116 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1999 2857 1010 2007 1996 3112 1997 1996 2034 2161 1997 8942 1024 1996 6579 2186 1010 6654 3428 4837 1996 2136 3625 2005 1996 2718 1011 2265 2007 5155 1037 3444 1011 3091 3185 1010 2761 18517 2005 3622 1011 2000 1011 2678 1010 2021 19030 2039 2000 8900 3570 1012 2009 2052 2468 2124 2004 8942 1024 7308 1997 1996 6887 26802 6491 1012 2702 2086 2044 6887 26802 6491 1010 2057 2031 2018 2019 3176 2093 3444 1011 3152 2207 2013 1996 3337 2012 1996 25610 1010 4942 1011 5717 1010 2709 1997 1996 19318 1010 1998 2085 1010 6547 1997 1996 7151 10169 9794 1996 2155 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5436 2003 3937 1998 1999 2116 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] has anyone been able to buy this movie ? my uncle \" hut ##ch \" was a real ( not reel ) pilot who is seen tossing his wings in the air and then snatch ##ing them with his fist as he was awarded his pilot ' s wings . < br / > < br / > he ' s only on screen a few seconds but my family would love to have the movie . he was killed in a dog ##fight over italy , he was only 24 at the time . do we know the film studio that made it ? < br / > < br / > or has anyone seen it at a video store , like blockbuster ? [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] has anyone been able to buy this movie ? my uncle \" hut ##ch \" was a real ( not reel ) pilot who is seen tossing his wings in the air and then snatch ##ing them with his fist as he was awarded his pilot ' s wings . < br / > < br / > he ' s only on screen a few seconds but my family would love to have the movie . he was killed in a dog ##fight over italy , he was only 24 at the time . do we know the film studio that made it ? < br / > < br / > or has anyone seen it at a video store , like blockbuster ? [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2038 3087 2042 2583 2000 4965 2023 3185 1029 2026 4470 1000 12570 2818 1000 2001 1037 2613 1006 2025 15934 1007 4405 2040 2003 2464 15021 2010 4777 1999 1996 2250 1998 2059 23365 2075 2068 2007 2010 7345 2004 2002 2001 3018 2010 4405 1005 1055 4777 1012 1026 7987 1013 1028 1026 7987 1013 1028 2002 1005 1055 2069 2006 3898 1037 2261 3823 2021 2026 2155 2052 2293 2000 2031 1996 3185 1012 2002 2001 2730 1999 1037 3899 20450 2058 3304 1010 2002 2001 2069 2484 2012 1996 2051 1012 2079 2057 2113 1996 2143 2996 2008 2081 2009 1029 1026 7987 1013 1028 1026 7987 1013 1028 2030 2038 3087 2464 2009 2012 1037 2678 3573 1010 2066 27858 1029 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2038 3087 2042 2583 2000 4965 2023 3185 1029 2026 4470 1000 12570 2818 1000 2001 1037 2613 1006 2025 15934 1007 4405 2040 2003 2464 15021 2010 4777 1999 1996 2250 1998 2059 23365 2075 2068 2007 2010 7345 2004 2002 2001 3018 2010 4405 1005 1055 4777 1012 1026 7987 1013 1028 1026 7987 1013 1028 2002 1005 1055 2069 2006 3898 1037 2261 3823 2021 2026 2155 2052 2293 2000 2031 1996 3185 1012 2002 2001 2730 1999 1037 3899 20450 2058 3304 1010 2002 2001 2069 2484 2012 1996 2051 1012 2079 2057 2113 1996 2143 2996 2008 2081 2009 1029 1026 7987 1013 1028 1026 7987 1013 1028 2030 2038 3087 2464 2009 2012 1037 2678 3573 1010 2066 27858 1029 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this tv - made thriller is all talk , little action . it works hard to set up its con ##vo ##lu ##ted plot , yet the writing is so mud ##dled the exposition is still cloudy at best . by the end , i knew no more about these characters than i did at the beginning . it has a quasi - \" ten little indians \" scenario , but ditch ##es it mid - way through in favor of spotlight ##ing sally field and her uncover ##ing of a killer . field over ##act ##s here with a grace ##lessness i ' ve seldom seen from her . the early introductions are good , but the writing quickly stray ##s off - course and [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this tv - made thriller is all talk , little action . it works hard to set up its con ##vo ##lu ##ted plot , yet the writing is so mud ##dled the exposition is still cloudy at best . by the end , i knew no more about these characters than i did at the beginning . it has a quasi - \" ten little indians \" scenario , but ditch ##es it mid - way through in favor of spotlight ##ing sally field and her uncover ##ing of a killer . field over ##act ##s here with a grace ##lessness i ' ve seldom seen from her . the early introductions are good , but the writing quickly stray ##s off - course and [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2694 1011 2081 10874 2003 2035 2831 1010 2210 2895 1012 2009 2573 2524 2000 2275 2039 2049 9530 6767 7630 3064 5436 1010 2664 1996 3015 2003 2061 8494 20043 1996 13080 2003 2145 24706 2012 2190 1012 2011 1996 2203 1010 1045 2354 2053 2062 2055 2122 3494 2084 1045 2106 2012 1996 2927 1012 2009 2038 1037 17982 1011 1000 2702 2210 6505 1000 11967 1010 2021 14033 2229 2009 3054 1011 2126 2083 1999 5684 1997 17763 2075 8836 2492 1998 2014 26944 2075 1997 1037 6359 1012 2492 2058 18908 2015 2182 2007 1037 4519 24913 1045 1005 2310 15839 2464 2013 2014 1012 1996 2220 25795 2024 2204 1010 2021 1996 3015 2855 15926 2015 2125 1011 2607 1998 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2694 1011 2081 10874 2003 2035 2831 1010 2210 2895 1012 2009 2573 2524 2000 2275 2039 2049 9530 6767 7630 3064 5436 1010 2664 1996 3015 2003 2061 8494 20043 1996 13080 2003 2145 24706 2012 2190 1012 2011 1996 2203 1010 1045 2354 2053 2062 2055 2122 3494 2084 1045 2106 2012 1996 2927 1012 2009 2038 1037 17982 1011 1000 2702 2210 6505 1000 11967 1010 2021 14033 2229 2009 3054 1011 2126 2083 1999 5684 1997 17763 2075 8836 2492 1998 2014 26944 2075 1997 1037 6359 1012 2492 2058 18908 2015 2182 2007 1037 4519 24913 1045 1005 2310 15839 2464 2013 2014 1012 1996 2220 25795 2024 2204 1010 2021 1996 3015 2855 15926 2015 2125 1011 2607 1998 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] \" who done it ? \" contains many sure ##fire laughs and gag ##s . it is not only one of the best st ##oo ##ges films with she ##mp , it is one of the finest short comedies columbia produced . director bern ##ds originally wrote the screenplay in 1946 for the st ##oo ##ges , but when curly howard suffered a career - ending stroke , it was tailored for the columbia comedy team of sc ##hill ##ing and lane ( their version was released as \" pardon my terror . \" ) it obviously was too good for the st ##oo ##ges to pass up , and it was finally filmed by the st ##oo ##ges at the end of 1947 . < [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] \" who done it ? \" contains many sure ##fire laughs and gag ##s . it is not only one of the best st ##oo ##ges films with she ##mp , it is one of the finest short comedies columbia produced . director bern ##ds originally wrote the screenplay in 1946 for the st ##oo ##ges , but when curly howard suffered a career - ending stroke , it was tailored for the columbia comedy team of sc ##hill ##ing and lane ( their version was released as \" pardon my terror . \" ) it obviously was too good for the st ##oo ##ges to pass up , and it was finally filmed by the st ##oo ##ges at the end of 1947 . < [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1000 2040 2589 2009 1029 1000 3397 2116 2469 10273 11680 1998 18201 2015 1012 2009 2003 2025 2069 2028 1997 1996 2190 2358 9541 8449 3152 2007 2016 8737 1010 2009 2003 2028 1997 1996 10418 2460 22092 3996 2550 1012 2472 16595 5104 2761 2626 1996 9000 1999 3918 2005 1996 2358 9541 8449 1010 2021 2043 17546 4922 4265 1037 2476 1011 4566 6909 1010 2009 2001 21727 2005 1996 3996 4038 2136 1997 8040 7100 2075 1998 4644 1006 2037 2544 2001 2207 2004 1000 14933 2026 7404 1012 1000 1007 2009 5525 2001 2205 2204 2005 1996 2358 9541 8449 2000 3413 2039 1010 1998 2009 2001 2633 6361 2011 1996 2358 9541 8449 2012 1996 2203 1997 4006 1012 1026 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1000 2040 2589 2009 1029 1000 3397 2116 2469 10273 11680 1998 18201 2015 1012 2009 2003 2025 2069 2028 1997 1996 2190 2358 9541 8449 3152 2007 2016 8737 1010 2009 2003 2028 1997 1996 10418 2460 22092 3996 2550 1012 2472 16595 5104 2761 2626 1996 9000 1999 3918 2005 1996 2358 9541 8449 1010 2021 2043 17546 4922 4265 1037 2476 1011 4566 6909 1010 2009 2001 21727 2005 1996 3996 4038 2136 1997 8040 7100 2075 1998 4644 1006 2037 2544 2001 2207 2004 1000 14933 2026 7404 1012 1000 1007 2009 5525 2001 2205 2204 2005 1996 2358 9541 8449 2000 3413 2039 1010 1998 2009 2001 2633 6361 2011 1996 2358 9541 8449 2012 1996 2203 1997 4006 1012 1026 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] a potentially good idea gets completely let down by a weak script which throws all credibility out the window leaving the actors very little to work with . roth covers it up as best he can by being all mouth ; hurt has about as much menace as a fluffy bunny and stamp can ' t seem to decide whether he ' s playing an ex - london crook or some to ##ff straight off the playing fields at eton . as for poor laura del sol she does what she can but her character is no more than every northern european ' s idea of the stereo ##typical latino woman who ' s all po ##uts and hot temperament . if you ' re a [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] a potentially good idea gets completely let down by a weak script which throws all credibility out the window leaving the actors very little to work with . roth covers it up as best he can by being all mouth ; hurt has about as much menace as a fluffy bunny and stamp can ' t seem to decide whether he ' s playing an ex - london crook or some to ##ff straight off the playing fields at eton . as for poor laura del sol she does what she can but her character is no more than every northern european ' s idea of the stereo ##typical latino woman who ' s all po ##uts and hot temperament . if you ' re a [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1037 9280 2204 2801 4152 3294 2292 2091 2011 1037 5410 5896 2029 11618 2035 21553 2041 1996 3332 2975 1996 5889 2200 2210 2000 2147 2007 1012 12211 4472 2009 2039 2004 2190 2002 2064 2011 2108 2035 2677 1025 3480 2038 2055 2004 2172 19854 2004 1037 27036 16291 1998 11359 2064 1005 1056 4025 2000 5630 3251 2002 1005 1055 2652 2019 4654 1011 2414 19302 2030 2070 2000 4246 3442 2125 1996 2652 4249 2012 21687 1012 2004 2005 3532 6874 3972 14017 2016 2515 2054 2016 2064 2021 2014 2839 2003 2053 2062 2084 2296 2642 2647 1005 1055 2801 1997 1996 12991 27086 7402 2450 2040 1005 1055 2035 13433 16446 1998 2980 26270 1012 2065 2017 1005 2128 1037 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1037 9280 2204 2801 4152 3294 2292 2091 2011 1037 5410 5896 2029 11618 2035 21553 2041 1996 3332 2975 1996 5889 2200 2210 2000 2147 2007 1012 12211 4472 2009 2039 2004 2190 2002 2064 2011 2108 2035 2677 1025 3480 2038 2055 2004 2172 19854 2004 1037 27036 16291 1998 11359 2064 1005 1056 4025 2000 5630 3251 2002 1005 1055 2652 2019 4654 1011 2414 19302 2030 2070 2000 4246 3442 2125 1996 2652 4249 2012 21687 1012 2004 2005 3532 6874 3972 14017 2016 2515 2054 2016 2064 2021 2014 2839 2003 2053 2062 2084 2296 2642 2647 1005 1055 2801 1997 1996 12991 27086 7402 2450 2040 1005 1055 2035 13433 16446 1998 2980 26270 1012 2065 2017 1005 2128 1037 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i recently picked up all three rob ##oco ##p films in one box set , rather cheap ##ly and the only reason i did this was for the special edition of the superb first one . i have seen rob ##oco ##p 2 before but not for 17 years , the year it came out . i have never watched it since because i can still remember how disappointed i was when i discovered how app ##all ##ing it really is . its a complete mess really , it has all the signs of a troubled production with so many sub - plots going on at the same time . it has a very uneven tone also and it is also one of the nas ##ties [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i recently picked up all three rob ##oco ##p films in one box set , rather cheap ##ly and the only reason i did this was for the special edition of the superb first one . i have seen rob ##oco ##p 2 before but not for 17 years , the year it came out . i have never watched it since because i can still remember how disappointed i was when i discovered how app ##all ##ing it really is . its a complete mess really , it has all the signs of a troubled production with so many sub - plots going on at the same time . it has a very uneven tone also and it is also one of the nas ##ties [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3728 3856 2039 2035 2093 6487 24163 2361 3152 1999 2028 3482 2275 1010 2738 10036 2135 1998 1996 2069 3114 1045 2106 2023 2001 2005 1996 2569 3179 1997 1996 21688 2034 2028 1012 1045 2031 2464 6487 24163 2361 1016 2077 2021 2025 2005 2459 2086 1010 1996 2095 2009 2234 2041 1012 1045 2031 2196 3427 2009 2144 2138 1045 2064 2145 3342 2129 9364 1045 2001 2043 1045 3603 2129 10439 8095 2075 2009 2428 2003 1012 2049 1037 3143 6752 2428 1010 2009 2038 2035 1996 5751 1997 1037 11587 2537 2007 2061 2116 4942 1011 14811 2183 2006 2012 1996 2168 2051 1012 2009 2038 1037 2200 17837 4309 2036 1998 2009 2003 2036 2028 1997 1996 17235 7368 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3728 3856 2039 2035 2093 6487 24163 2361 3152 1999 2028 3482 2275 1010 2738 10036 2135 1998 1996 2069 3114 1045 2106 2023 2001 2005 1996 2569 3179 1997 1996 21688 2034 2028 1012 1045 2031 2464 6487 24163 2361 1016 2077 2021 2025 2005 2459 2086 1010 1996 2095 2009 2234 2041 1012 1045 2031 2196 3427 2009 2144 2138 1045 2064 2145 3342 2129 9364 1045 2001 2043 1045 3603 2129 10439 8095 2075 2009 2428 2003 1012 2049 1037 3143 6752 2428 1010 2009 2038 2035 1996 5751 1997 1037 11587 2537 2007 2061 2116 4942 1011 14811 2183 2006 2012 1996 2168 2051 1012 2009 2038 1037 2200 17837 4309 2036 1998 2009 2003 2036 2028 1997 1996 17235 7368 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] there ##s not much you can really say about this film except that it was crap and probably the worst film i have ever been to see ! ! take my advice don ' t watch this film it just waste ##s your money and time ! ! < br / > < br / > i gave this film a 1 / 10 which is doesn ' t deserve . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] there ##s not much you can really say about this film except that it was crap and probably the worst film i have ever been to see ! ! take my advice don ' t watch this film it just waste ##s your money and time ! ! < br / > < br / > i gave this film a 1 / 10 which is doesn ' t deserve . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2045 2015 2025 2172 2017 2064 2428 2360 2055 2023 2143 3272 2008 2009 2001 10231 1998 2763 1996 5409 2143 1045 2031 2412 2042 2000 2156 999 999 2202 2026 6040 2123 1005 1056 3422 2023 2143 2009 2074 5949 2015 2115 2769 1998 2051 999 999 1026 7987 1013 1028 1026 7987 1013 1028 1045 2435 2023 2143 1037 1015 1013 2184 2029 2003 2987 1005 1056 10107 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2045 2015 2025 2172 2017 2064 2428 2360 2055 2023 2143 3272 2008 2009 2001 10231 1998 2763 1996 5409 2143 1045 2031 2412 2042 2000 2156 999 999 2202 2026 6040 2123 1005 1056 3422 2023 2143 2009 2074 5949 2015 2115 2769 1998 2051 999 999 1026 7987 1013 1028 1026 7987 1013 1028 1045 2435 2023 2143 1037 1015 1013 2184 2029 2003 2987 1005 1056 10107 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] finally a gangster movie worth watching ! < br / > < br / > jennifer til ##y should get nominated for her role as tough murdering femme fatal ! < br / > < br / > this movie flies like a bird , just a fast paced non stop gangster mayhem ! < br / > < br / > jennifer til ##y is just so beautiful and bad in this movie . < br / > < br / > i was shocked to find faye dun ##away still lives ! the cast in this movie are so fitted to their roles . < br / > < br / > a real cool soundtrack rides along side and you get swept into [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] finally a gangster movie worth watching ! < br / > < br / > jennifer til ##y should get nominated for her role as tough murdering femme fatal ! < br / > < br / > this movie flies like a bird , just a fast paced non stop gangster mayhem ! < br / > < br / > jennifer til ##y is just so beautiful and bad in this movie . < br / > < br / > i was shocked to find faye dun ##away still lives ! the cast in this movie are so fitted to their roles . < br / > < br / > a real cool soundtrack rides along side and you get swept into [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2633 1037 20067 3185 4276 3666 999 1026 7987 1013 1028 1026 7987 1013 1028 7673 18681 2100 2323 2131 4222 2005 2014 2535 2004 7823 21054 26893 10611 999 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 10029 2066 1037 4743 1010 2074 1037 3435 13823 2512 2644 20067 26865 999 1026 7987 1013 1028 1026 7987 1013 1028 7673 18681 2100 2003 2074 2061 3376 1998 2919 1999 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2001 7135 2000 2424 19243 24654 9497 2145 3268 999 1996 3459 1999 2023 3185 2024 2061 7130 2000 2037 4395 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2613 4658 6050 12271 2247 2217 1998 2017 2131 7260 2046 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2633 1037 20067 3185 4276 3666 999 1026 7987 1013 1028 1026 7987 1013 1028 7673 18681 2100 2323 2131 4222 2005 2014 2535 2004 7823 21054 26893 10611 999 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 10029 2066 1037 4743 1010 2074 1037 3435 13823 2512 2644 20067 26865 999 1026 7987 1013 1028 1026 7987 1013 1028 7673 18681 2100 2003 2074 2061 3376 1998 2919 1999 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2001 7135 2000 2424 19243 24654 9497 2145 3268 999 1996 3459 1999 2023 3185 2024 2061 7130 2000 2037 4395 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2613 4658 6050 12271 2247 2217 1998 2017 2131 7260 2046 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this was a movie of which i kept on reading the reviews again and again ; and despite it being played at film museum and not at path ##e theatres i decided to give this movie a try . the reasons were many in the reviews it was compared with pulp fiction , it had several parallel stories running in the movie and lastly it had already won 17 awards internationally in various categories . i was eager to see this movie and due to my off day at green ##pe ##ace i decided to make myself happy by going and seeing this movie . < br / > < br / > it is a story based in finland . i think it reflected the [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this was a movie of which i kept on reading the reviews again and again ; and despite it being played at film museum and not at path ##e theatres i decided to give this movie a try . the reasons were many in the reviews it was compared with pulp fiction , it had several parallel stories running in the movie and lastly it had already won 17 awards internationally in various categories . i was eager to see this movie and due to my off day at green ##pe ##ace i decided to make myself happy by going and seeing this movie . < br / > < br / > it is a story based in finland . i think it reflected the [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2001 1037 3185 1997 2029 1045 2921 2006 3752 1996 4391 2153 1998 2153 1025 1998 2750 2009 2108 2209 2012 2143 2688 1998 2025 2012 4130 2063 13166 1045 2787 2000 2507 2023 3185 1037 3046 1012 1996 4436 2020 2116 1999 1996 4391 2009 2001 4102 2007 16016 4349 1010 2009 2018 2195 5903 3441 2770 1999 1996 3185 1998 22267 2009 2018 2525 2180 2459 2982 7587 1999 2536 7236 1012 1045 2001 9461 2000 2156 2023 3185 1998 2349 2000 2026 2125 2154 2012 2665 5051 10732 1045 2787 2000 2191 2870 3407 2011 2183 1998 3773 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2003 1037 2466 2241 1999 6435 1012 1045 2228 2009 7686 1996 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2001 1037 3185 1997 2029 1045 2921 2006 3752 1996 4391 2153 1998 2153 1025 1998 2750 2009 2108 2209 2012 2143 2688 1998 2025 2012 4130 2063 13166 1045 2787 2000 2507 2023 3185 1037 3046 1012 1996 4436 2020 2116 1999 1996 4391 2009 2001 4102 2007 16016 4349 1010 2009 2018 2195 5903 3441 2770 1999 1996 3185 1998 22267 2009 2018 2525 2180 2459 2982 7587 1999 2536 7236 1012 1045 2001 9461 2000 2156 2023 3185 1998 2349 2000 2026 2125 2154 2012 2665 5051 10732 1045 2787 2000 2191 2870 3407 2011 2183 1998 3773 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 2003 1037 2466 2241 1999 6435 1012 1045 2228 2009 7686 1996 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"cell_type":"code","metadata":{"id":"6o2a5ZIvRcJq","colab_type":"code","colab":{}},"source":["def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"code","metadata":{"id":"FnH-AnOQ9KKW","colab_type":"code","colab":{}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEJldMr3WYZa","colab_type":"code","colab":{}},"source":["# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_WebpS1X97v","colab_type":"code","outputId":"5a20738e-6e54-4431-8d9a-8466f17afcb0","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1573068899115,"user_tz":-60,"elapsed":94075,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77acc0e630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77acc0e630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NOO3RfG1DYLo","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"cell_type":"code","metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6Nukby2EB6-","colab_type":"text"},"source":["Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."]},{"cell_type":"code","metadata":{"id":"nucD4gluYJmK","colab_type":"code","outputId":"d848271d-8aae-4c02-a500-4337e3a7ec1f","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1573068899117,"user_tz":-60,"elapsed":94047,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Skipping training since max_steps has already saved.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Skipping training since max_steps has already saved.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:00:00.011980\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CmbLTVniARy3","colab_type":"text"},"source":["Now let's use our test data to see how well our model did:"]},{"cell_type":"code","metadata":{"id":"JIhejfpyJ8Bx","colab_type":"code","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPVEXhNjYXC-","colab_type":"code","outputId":"ca252685-215a-4276-d3ec-a8ce690bee7f","colab":{"base_uri":"https://localhost:8080/","height":632},"executionInfo":{"status":"ok","timestamp":1573068998608,"user_tz":-60,"elapsed":193494,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-06T19:35:11Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-06T19:35:11Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-06-19:36:37\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-06-19:36:37\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.87170166, eval_accuracy = 0.8716, f1_score = 0.8715999, false_negatives = 348.0, false_positives = 294.0, global_step = 468, loss = 0.49387643, precision = 0.88111603, recall = 0.8622873, true_negatives = 2179.0, true_positives = 2179.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.87170166, eval_accuracy = 0.8716, f1_score = 0.8715999, false_negatives = 348.0, false_positives = 294.0, global_step = 468, loss = 0.49387643, precision = 0.88111603, recall = 0.8622873, true_negatives = 2179.0, true_positives = 2179.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.87170166,\n"," 'eval_accuracy': 0.8716,\n"," 'f1_score': 0.8715999,\n"," 'false_negatives': 348.0,\n"," 'false_positives': 294.0,\n"," 'global_step': 468,\n"," 'loss': 0.49387643,\n"," 'precision': 0.88111603,\n"," 'recall': 0.8622873,\n"," 'true_negatives': 2179.0,\n"," 'true_positives': 2179.0}"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"ueKsULteiz1B","colab_type":"text"},"source":["Now let's write code to make predictions on new sentences:"]},{"cell_type":"code","metadata":{"id":"OsrbTD2EJTVl","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  labels = [\"Negative\", \"Positive\"]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n","  predictions = estimator.predict(predict_input_fn)\n","  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-thbodgih_VJ","colab_type":"code","colab":{}},"source":["pred_sentences = [\n","  \"That movie was absolutely awful\",\n","  \"The acting was a bit lacking\",\n","  \"The film was creative and surprising\",\n","  \"Absolutely fantastic!\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrZmvZySKQTm","colab_type":"code","outputId":"a529ce5e-7927-4238-e44f-2da975448a06","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1573069003936,"user_tz":-60,"elapsed":198604,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["predictions = getPrediction(pred_sentences)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MXkRiEBUqN3n","colab_type":"text"},"source":["Voila! We have a sentiment classifier!"]},{"cell_type":"code","metadata":{"id":"ERkTE8-7oQLZ","colab_type":"code","outputId":"d76f469b-0bc3-4a93-b437-782343fcfb1f","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1573070694866,"user_tz":-60,"elapsed":1717,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["predictions"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('That movie was absolutely awful',\n","  array([-2.9186292e-03, -5.8381004e+00], dtype=float32),\n","  'Negative'),\n"," ('The acting was a bit lacking',\n","  array([-0.06608613, -2.7496579 ], dtype=float32),\n","  'Negative'),\n"," ('The film was creative and surprising',\n","  array([-5.251999e+00, -5.250830e-03], dtype=float32),\n","  'Positive'),\n"," ('Absolutely fantastic!',\n","  array([-4.9743705 , -0.00693687], dtype=float32),\n","  'Positive')]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"64jRhoUyUeKN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1afed86f-bd7a-48b6-ff83-a8cdc3188177","executionInfo":{"status":"ok","timestamp":1573070803331,"user_tz":-60,"elapsed":6776,"user":{"displayName":"Alessandro Pedori","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZ_QXse8X3tRezZL0S4NOPVCPk_yWFQK_TNyprXA=s64","userId":"12717156193525782223"}}},"source":["pred_strange = [\n","  \"That movie was not that horrible\",\n","  \"The acting was better than watching broomsticks, but not much\",\n","  \"The film was not a masterpiece, but good enough\",\n","  \"The film was a masterpiece of really woody acting!\"\n","]\n","predictions_strange = getPrediction(pred_strange)\n","predictions_strange"],"execution_count":64,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was not that horrible [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was not that horrible [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 2025 2008 9202 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 2025 2008 9202 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was better than watching broom ##stick ##s , but not much [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was better than watching broom ##stick ##s , but not much [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 2488 2084 3666 23528 21354 2015 1010 2021 2025 2172 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 2488 2084 3666 23528 21354 2015 1010 2021 2025 2172 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was not a masterpiece , but good enough [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was not a masterpiece , but good enough [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 2025 1037 17743 1010 2021 2204 2438 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 2025 1037 17743 1010 2021 2204 2438 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was a masterpiece of really woody acting ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was a masterpiece of really woody acting ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 1037 17743 1997 2428 13703 3772 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 1037 17743 1997 2428 13703 3772 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('That movie was not that horrible',\n","  array([-0.00572684, -5.1654644 ], dtype=float32),\n","  'Negative'),\n"," ('The acting was better than watching broomsticks, but not much',\n","  array([-0.03051801, -3.5046604 ], dtype=float32),\n","  'Negative'),\n"," ('The film was not a masterpiece, but good enough',\n","  array([-3.8856697 , -0.02074777], dtype=float32),\n","  'Positive'),\n"," ('The film was a masterpiece of really woody acting!',\n","  array([-5.1787834 , -0.00565074], dtype=float32),\n","  'Positive')]"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"dLkb8lbMU3a2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}